---
title: '统计学入门'
date: 2025-09-03
permalink: /posts/2025/09/2025-09-03-统计学入门/
excerpt: "微积分视角的统计学入门。"
tags:
  - Math
  - Stat
---

笔者没有学过数学分析等知识，所以全文的内容应该有很大一部分是不严格的。当成感性理解看吧。

# 统计学的基本框架

- 描述：通过一些参数来描述总体的特征。
- 推断：基于已有的数据（样本），推断总体的规律，甚至作出预测。

# 概率分布

对于一个连续型随机变量 $X$，若有一函数 $f$ 恒非负，且满足对于任意区间 $[l,r]$，$P(X \in [l,r]) = \int_l^r f(x) dx$，则称 $f$ 是 $X$ 的**概率密度函数（Probability density function, PDF）**。

所有事件的概率和为 $1$，因此 PDF 满足以下性质：

$$\int_{-\infty}^\infty f(x) dx = 1$$

**累积分布函数（Cumulative distribution function, CDF）** 是 PDF 的原函数：

$$F(x) = \int_{-\infty}^x f(x) dx$$

**概率分布**描述不同事件发生的可能性，每一种概率分布有自己的 PDF。若我们有一个分布 $D$，我们想说 $X$ 服从这个分布，可以用这个符号：

$$X \sim D$$

对于常见的分布，我们还有特别的记号，见后文的常见概率分布。

除了**连续概率分布**，还有**离散概率分布**。这就更简单了，把所有 $\int$ 改成 $\sum$ 就行咯。对于离散分布我们一般不说 PDF，而是叫做**概率质量函数（Probability mass function, PMF）**，但是意思是一样的。

## 期望 Expectation

$$\mathbb E[X] = \int_{-\infty}^\infty x f(x) dx$$

直觉建立：和物理学中的**重心**是相似的。

$$x_{\text{CM}} = \frac {\int x \rho(x) dx} {\int \rho(x) dx}$$

分母上的总质量在概率的语境下是总概率，永远是 $1$。

## 方差 Variance

（注：AP 课本使用 $\text{var}$，但是互联网上的其他资料均使用 $\text{Var}$。所以可能会混用。）

$$\text{Var}(X) = \mathbb E[(X - \mathbb E[X])^2]$$

> $\text{Var}(X)$ 还会记作 $\sigma^2$。$\sigma$ 是**标准差（standard deviation）**，定义为方差的非负平方根 $\sqrt{\text{Var}(X)}$。还有一种写法是 $\sigma = \text{SD}(X)$（AP 课本的写法）。
>
> 若 $X$ 满足期望为 $\mu$ 标准差为 $\sigma$ 的分布，则 $\frac {X - \mu} \sigma$ 满足期望为 $0$ 标准差为 $1$ 的分布。$\frac {X - \mu} \sigma$ 被称为 **Z 分数（z-score）**，也叫**标准分（standard score）**。

直觉建立：

$$\text{Var}(X) = \int (x - \mathbb E[X])^2 f(x) dx$$

和物理学中的**转动惯量**是相似的，这里特指轴过重心时的转动惯量。转动惯量和方差都描述了分布的分散程度，一个是物理的密度，另一个是概率的密度。

根据这样的直觉，我们可以发现**平行轴定理**的类比：

$$\mathbb E[(X - a)^2] = \text{Var}(X) + (\mathbb E[X] - a)^2$$

令 $a=0$ 我们可以得到一个方差的常用公式：

$$\text{Var}(X) = \mathbb E[X^2] - \mathbb E[X]^2$$

### 协方差 Covariance

仿照物理中**惯量积**（即 $\int xy dm = \int xy \rho(x,y) dx dy$）的定义，我们定义 $\text{Cov}(X,Y)$ 是 $X,Y$ 的**协方差**：

$$\text{Cov}(X,Y) = \int_{-\infty}^\infty \int_{-\infty}^\infty (x - \mathbb E[X]) (y - \mathbb E[Y]) f(x,y) dx dy$$

即

$$\text{Cov}(X,Y) = \mathbb E[(X - \mathbb E[X]) (Y - \mathbb E[Y])]$$

仿照前面的平行轴定理我们可以写出类似的公式：

$$\text{Cov}(X,Y) = \mathbb E[XY] - \mathbb E[X] \mathbb E[Y]$$

---

注意到，根据协方差定义，$\text{Cov}(X,Y) > 0$ 意味着当 $X > \mathbb E[X]$ 时经常 $Y > \mathbb E[Y]$，反之亦然，这体现了一种“正相关”性。同理 $\text{Cov}(X,Y) < 0$ 体现了一种“负相关性”。

但是用协方差来描述相关性不太好，因为带量纲。我们除掉两个标准差化为无量纲数，称为**总体 Pearson 相关系数**：

$$\rho_{X,Y} = \frac {\text{Cov}(X, Y)} {\text{SD}(X) \text{SD}(Y)}$$

取值是 $\rho_{X,Y} \in [-1, 1]$。

### 协方差矩阵

仿照物理中**转动惯量张量**，我们定义**协方差**矩阵（以二维为例）：

$$\Sigma = \begin{bmatrix}
    \text{Var}(X) & \text{Cov}(X,Y) \\
    \text{Cov}(Y,X) & \text{Var}(Y) \\
\end{bmatrix}$$

和转动惯量张量一样，对于一个单位向量 $\vec{u}$，我们可以得到这个方向上 $\vec{X}$ 的方差：

$$\text{Var}(\vec{u}^T \vec{X}) = \vec{u}^T \Sigma \vec{u}$$

## 期望和方差的常见性质

### 期望

- 对于两个随机变量 $X,Y$（**不要求独立**），有**期望的线性性**：

$$\mathbb E[aX + bY] = a \mathbb E[X] + b \mathbb E[Y]$$

- 对于两个**独立**的随机变量 $X,Y$，有：

$$\mathbb E[XY] = \mathbb E[X] \mathbb E[Y]$$

对于非独立的情况：$\mathbb E[XY] = \mathbb E[X] \mathbb E[Y] + \text{Cov}(X, Y)$

### 方差

- 平移一个常数 $C$：

$$\text{Var}(X + C) = \text{Var}(X)$$

- 数乘：

$$\text{Var}(aX) = a^2 \text{Var}(X)$$

- 对于**独立**的随机变量 $X,Y$，有：

$$\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y)$$

证明：

$$\begin{aligned}
    \text{Var}(X + Y) &= \mathbb E[(X+Y)^2] - \mathbb E[X+Y]^2 \\
    &= \mathbb E[X^2] + 2 \mathbb E[XY] + \mathbb E[Y^2] - \mathbb E[X]^2 - 2 \mathbb E[X] \mathbb E[Y] - \mathbb E[Y]^2 \\
    &= \text{Var}(X) + \text{Var}(Y) + 2 (\mathbb E[XY] - \mathbb E[X] \mathbb E[Y]) \\
    &= \text{Var}(X) + \text{Var}(Y) \\
\end{aligned}$$

对于非独立的情况：$\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) + 2 \text{Cov}(X, Y)$

### 协方差

- 可交换：

$$\text{Cov}(X,Y) = \text{Cov}(Y,X)$$

- 数乘：

$$\text{Cov}(aX, bY) = ab \text{Cov}(Y,X)$$

- $\text{Cov}$ 对加法的分配律：

$$\text{Cov}(X + Y, Z) = \text{Cov}(X, Z) + \text{Cov}(Y, Z)$$

## 条件期望

*这段是写完最小二乘法回来补的，可能有点突兀*。

$$\mathbb E[y \mid x]$$

是一个关于 $x$ 的函数，表示在 $x$ 确定时 $y$ 的期望值。

### 期望迭代定理

$$\mathbb E[Y] = \mathbb E[\mathbb E[Y \mid X]]$$

证明：

$$\begin{aligned}
    & \mathbb E[\mathbb E[Y \mid X]] \\
    =& \int \mathbb E[Y \mid X = x] f_X(x) dx \\
    =& \int \int y f_{Y \mid X}(y \mid X = x) dy f_X(x) dx \\
    =& \int y \left(\int f_{Y \mid X}(y \mid X = x) f_X(x) dx \right) dy \\
    =& \int y f_Y(y) dy \\
    =& \mathbb E[Y] \\
\end{aligned}$$

## 常见的概率分布

见[另一篇文章](https://august-light.github.io/posts/2025/09/2025-09-03-%E5%B8%B8%E8%A7%81%E7%9A%84%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83/)。实践性较强，与本文的理论关系不大。

## References

- [概率论学习笔记（一）](https://zhuanlan.zhihu.com/p/42859784)
- [概率论学习笔记（二）](https://zhuanlan.zhihu.com/p/42950143)
- [概率论学习笔记（四）](https://zhuanlan.zhihu.com/p/45720457)

# 总体和样本

刚才说到的 $\mu,\sigma$ 这些量，都是隐藏在现实世界背后的真正规律，是**总体**的量。在实际生活中，我们只能采集有限的样本，来试图逼近这些真实的总体的量。通过样本进行的估算，得到的是**样本**的量。我们使用记号 `\hat` 来说明一个总体量是被估算出来的，比如 $\hat \mu, \hat \sigma$。

$\mathbb E[\hat{\mu}] = \mu$，$\mathbb E[\hat{\sigma}^2] = \sigma^2$。这种通过保证期望正确来估计的方式称为**无偏估计**。

符号滥用：这一部分会混用 $x$ 和 $X$。

## 均值

总体均值记作 $\mu$，**样本均值**记作 $\bar{x}$（$\hat \mu$）。

接下来我们推导 $\bar{x}$ 的表达式。令 $X_i$ 为 $X$ 的第 $i$ 次实现值。

$$\begin{aligned}
    \mu =& \mathbb E[X] \\
    =& \frac 1 n \sum\limits_{i=1}^n \mathbb E[X_i] \\
    =& \mathbb E \left[ \frac 1 n \sum\limits_{i=1}^n X_i \right] \\
\end{aligned}$$

$$\boxed{\hat \mu = \bar X = \frac 1 n \sum\limits_{i=1}^n X_i}$$

## 方差

总体方差记作 $\sigma^2$，**（无偏）样本方差**记作 $s^2$（$\hat \sigma^2$）。

$\sigma$ 是用 $\mu$ 计算的，$\hat \sigma$ 得要转成用 $\bar{x}$ 计算的形式。

$$\begin{aligned}
    & \sigma^2 \\
    =& \mathbb E \left[ \sum (x - \mu)^2 \right] \\
    =& \mathbb E \left[ \frac 1 n \sum (X_i - \mu)^2 \right] \\
    =& \frac 1 n \mathbb E \left[ \sum (X_i - \bar{X} + \bar{X} - \mu)^2 \right] \\
    =& \frac 1 n \mathbb E \left[ \sum (X_i - \bar{X})^2 + 2 (\bar{X} - \mu) \sum (X_i - \bar{X}) + \sum (\bar{X} - \mu)^2 \right] \\
    =& \frac 1 n \mathbb E \left[ \sum (X_i - \bar{X})^2 + n (\bar{X} - \mu)^2 \right] \\
    =& \frac 1 n \mathbb E \left[ \sum (X_i - \bar{X})^2 \right] + \frac 1 n \times n \times \mathbb E[(\bar{X} - \mathbb E[\bar{X}])^2] & ^* \mu = \mathbb E[\bar{X}] \\
    =& \frac 1 n \mathbb E \left[ \sum (X_i - \bar{X})^2 \right] + \frac 1 n \times n \times \text{Var}(\bar{X}) \\
    =& \frac 1 n \mathbb E \left[ \sum (X_i - \bar{X})^2 \right] + \frac {\sigma^2} n \\
\end{aligned}$$

因此 $\frac 1 n \mathbb E \left[ \sum (X_i - \bar{X})^2 \right] = \frac {n-1} n \sigma^2$，即

$$\sigma^2 = \mathbb E \left[ \frac 1 {n-1} \sum (X_i - \bar{X})^2 \right]$$

$$\boxed{\hat{\sigma}^2 = s^2 = \frac 1 {n-1} \sum\limits_{i=1}^n (X_i - \bar{X})^2}$$

这个除以 $n-1$ 初次见到可能有点反直觉。如果不是除以 $n-1$ 而是除以 $n$，这样得到的方差估计是**有偏样本方差**。这样算出来的有偏估计 $s_{\text{biased}}^2$ 满足 $\mathbb E[s_{\text{biased}}^2] = \frac{n-1}{n} \sigma^2$。在“似然”一节中，我们会讨论有偏样本方差的价值。

## 协方差

样本方差要除以 $n-1$，可以类比得到（无偏）样本协方差也要除以 $n-1$：

$$s^2 = \frac 1 {n-1} \sum\limits_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})$$

同理，ChatGPT 说除以 $n$ 的版本得到的有偏协方差是正态分布数据下 MLE 的。

### 样本 Pearson 相关系数

$$\hat{\rho}_{X,Y} = r = \frac {\widehat{\text{Cov}}(X,Y)} {\widehat{\text{SD}}(X) \widehat{\text{SD}}(Y)}$$

或者一个更常见的形式：

$$r = \frac {\sum\limits_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})} {\sqrt{\sum\limits_{i=1}^n (x_i - \bar{x})^2 \sum\limits_{i=1}^n (y_i - \bar{y})^2}}$$

AP 书上写的版本是：

$$r = \frac 1 {n-1} \sum\limits_{i=1}^n \left( \frac {x_i - \bar{x}} {\widehat{\text{SD}}(X)} \right) \left( \frac {y_i - \bar{y}} {\widehat{\text{SD}}(Y)} \right)$$

本质相同，但是是从 Z 分数的视角来看的。

# 估计

## 有偏样本方差是没有用的吗？

有偏方差连期望都不对，虽然公式很好看，但是它难道不是错的吗？

介绍一个概念叫做**似然（likelihood）**。我们已经通过实验收集了一个样本，现在我们想知道当参数取某个值时，出现这个样本的概率为多大，这个概率称为似然。

比如，掷了 $10$ 次硬币有 $4$ 个正面，硬币正面概率为 $p$。似然：

$$L(p) = \binom {10} 4 p^4 (1 - p)^6$$

由此有一种思想叫做**最大似然估计（MLE）**，即已有一个观测数据出现时，找到最容易导致这个观测数据出现的参数。写成公式：

$$\hat{p} = \arg \max\limits_{p} L(p)$$

对于前面那个例子，可以算出 $\hat{p} = 0.4$，符合我们的直觉。

回到有偏样本方差。对于一组数据 $x_1, \cdots, x_n$ 来自 $\mathcal N(\mu, \sigma^2)$ i.i.d.，**在 MLE 意义下有偏方差是对总体方差的最好估计**。具体推导：

$$L(\mu, \sigma^2) = \prod\limits_{i=1}^n \frac 1 {\sqrt {2 \pi \sigma^2}} \exp\left( - \frac {(x_i - \mu)^2} {2 \sigma^2} \right)$$

为了方便计算，取对数：

$$\ln L (\mu, \sigma^2) = - \frac n 2 \ln (2 \pi \sigma^2) - \frac 1 {2 \sigma^2} \sum\limits_{i=1}^n (x_i - \mu)^2$$

（这一段机械计算略去）$\frac {\partial (\ln L)} {\partial \mu}$ 求一下可知 $\mu = \bar{x}$ 的时候是最优的，再求一下 $\frac {\partial (\ln L)} {\partial (\sigma^2)}$ 可知最优的是：

$$\sigma^2 = \frac 1 n \sum\limits_{i=1}^n (x_i - \bar{x})^2$$

也就是有偏的样本方差。

总之，**有偏的样本方差在数据满足正态分布 i.i.d. 时是满足 MLE 的**。这个结论在**最小二乘法**也有类似的形式。

## 线性回归 - 最小二乘法

我有一堆数据 $(x_i, y_i)$，我想用一根直线 $y = a + b x$ 来拟合，即 $y_i = a + b x_i + u_i$（$u_i$ 是噪声），使得这条直线拟合得最好。即我们要找到参数 $\hat{a}, \hat{b}$，使得

$$y_i = \hat{a} + \hat{b} x_i + \hat{u}_i$$

中的噪声 $\hat{u}_i$ 要某种程度上尽量小。

噪声 $u_i$ 满足 $\mathbb E[u_i \mid x_i] = 0$（这并不意味着 $u_i$ 与 $x_i$ 独立！只是说 $x_i$ 不影响 $\mathbb E[u_i]$，且 $x_i$ 指定时 $\mathbb E[u_i] = 0$）。

### MLE 解法

增加一个假设：$u_i \sim \mathcal N(0, \sigma^2)$ i.i.d.。

仿照和样本方差有偏估计一样地方法，对似然函数求对数，我们可以发现最大化似然相当于最小化平方误差和（计算过程省略）：

$$\arg \min\limits_{a,b} \sum\limits_{i=1}^n (y_i - (a + b x_i))^2$$

求偏导：

$$\begin{cases}
    \sum\limits_{i=1}^n (y_i - \hat{a} - \hat{b} x_i) = 0 \\
    \sum\limits_{i=1}^n (y_i - \hat{a} - \hat{b} x_i) x_i = 0 \\
\end{cases}$$

解方程组可得：

$$\begin{cases}
    \hat{b} = \dfrac {\sum\limits_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})} {\sum\limits_{i=1}^n (x_i - \bar{x})^2} = \dfrac {\text{Cov}(x,y)} {\text{Var}(x)} \\
    \hat{a} = \bar{y} - \bar{x} \hat{b} \\
\end{cases}$$

在 AP 考试中不涉及协方差，一般表示为：

$$\hat{b} = r \frac {\widehat{\text{SD}}(y)} {\widehat{\text{SD}}(x)} = r \frac {s_y} {s_x}$$

其中 $r$ 是样本的 Pearson 相关系数。

### Method of Moments 解法

*tmd 这段居然是学校课上讲的东西。怎么这么难*。

MoM 的思想是：样本的某些矩应该等于总体的某些矩。通过强行让这些矩相等，可以解出参数。

它倒是和 MLE 没有关系，但是值得注意的是，对于**线性回归**它能够和 MLE 解出一样的解，且**不需要正态噪声假设**。

我们知道关于噪声的以下信息。根据 $\mathbb E[u \mid x] = 0$ 的假设，可以通过**期望迭代定理**得到 $\mathbb E[u] = 0$ 和 $\mathbb E[ux] = 0$：

$$\mathbb E[u] = \mathbb E[\mathbb E[u \mid x]] = 0$$

$$\mathbb E[ux] = \mathbb E[\mathbb E[ux \mid x]] = \mathbb E[x \mathbb E[u \mid x]] = 0$$

即对于任意 $i$，有：

$$\begin{cases}
    \mathbb E[u_i] = 0 \\
    \mathbb E[u_i x_i] = 0 \\
\end{cases}$$

这是总体的矩，让样本的矩与其相等，即为：

$$\begin{cases}
    \mathbb E[\hat{u}_i] = 0 \\
    \mathbb E[\hat{u}_i x_i] = 0 \\
\end{cases}$$

$$\begin{cases}
    \sum\limits_{i=1}^n (y_i - \hat{a} - \hat{b} x_i) = 0 \\
    \sum\limits_{i=1}^n (y_i - \hat{a} - \hat{b} x_i) x_i = 0 \\
\end{cases}$$

得到的其实就是刚才 MLE 中偏导得到的表达式，和最小二乘法相同。

### AP 考试会考到的性质

- $\sum\limits_{i=1}^n \hat{u}_i = 0$
- $\sum\limits_{i=1}^n x_i \hat{u}_i = 0$
- 点 $(\bar{x}, \bar{y})$ 一定在最优直线上。
  - $\bar{\hat{y}} = \bar{y} = \hat{a} + \hat{b} \bar{x}$

### MoM 和 MLE 算出来的答案一定一样吗？

其实没啥关联 hhh。

举一个不一样的例子：有若干个 $x$ 独立取自均匀分布 $\mathcal U(0, \theta)$。现在要估算 $\theta$。
- MoM：$\mathbb E[x] = \frac \theta 2$，所以 $\hat \theta = 2 \bar{x}$。
- MLE：$L(\theta) = \prod\limits_{i=1}^n \frac {[0 \le x_i \le \theta]} \theta$，即 $\theta \ge \max\limits_{i=1}^n (x_i)$ 时是 $\theta^{-n}$，否则是 $0$。所以 $\hat \theta = \max(x_i)$。
- Unbiased：ChatGPT 说 $\hat \theta = \frac {n+1} n \max(x_i)$，我懒得算了。

**但是，线性回归问题中的 MoM、MLE 算出来的结果都是最小二乘法，同时最小二乘法还满足无偏**，由此体现了最小二乘的重要性。

TODO: 最小二乘法无偏

### 决定系数 $r^2$

**决定系数（coefficient of determination）**

https://en.wikipedia.org/wiki/Coefficient_of_determination

TODO:

## Logistic Regression

在机器学习领域，除了最小二乘法用到 MLE 以外，Logistic 回归也是使用 MLE 的经典场景。

我有一堆数据 $(\vec{x}_i, y_i)$，其中 $y_i \in \{0,1\}$，代表 $\vec{x}_i$ 被归到哪类。我希望找到一个函数 $h_\theta(\vec{x})$，代表我预测的 $\vec{x}$ 被归类到 $1$ 的概率，而且使得 $h_\theta(\vec{x}_i)$ 尽量接近 $y_i$。

对于一组给定的参数 $\vec{\theta}$，我们预测 $\vec{x}$ 被归类为 $1$ 的概率为

$$h_\theta(\vec{x}) = \sigma(\vec{\theta}^T \vec{x}) = \frac 1 {1 + e^{-\vec{\theta}^T \vec{x}}}$$

（$\sigma$ 为 Sigmoid 函数 $\frac 1 {1 + e^{-x}}$，值域 $[0,1]$ 单调递增）

我们希望找到一个最好的 $\vec{\theta}$，最好用 MLE 定义。似然即为

$$L_i(\vec{\theta}) = \begin{cases}
    h_\theta(\vec{x}) & y_i = 1 \\
    1 - h_\theta(\vec{x}) & y_i = 0 \\
\end{cases}$$

$$L(\vec{\theta}) = \prod\limits_{i=1}^n L_i(\vec{\theta}) = \prod\limits_{i=1}^n h_\theta(\vec{x}_i)^{y_i} (1 - h_\theta(\vec{x}_i))^{1 - y_i}$$

为计算方便取对数：

$$\ln L(\vec{\theta}) = \sum\limits_{i=1}^n (y_i \ln h_\theta(\vec{x}_i) + (1 - y_i) \ln (1 - h_\theta(\vec{x}_i)))$$

这就是我们熟悉的 Logistic regression 的 Cost function 形式。