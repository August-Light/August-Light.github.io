---
title: '统计学入门'
date: 2025-09-03
permalink: /posts/2025/09/2025-09-03-统计学入门/
excerpt: "微积分视角的统计学入门。"
tags:
  - Math
  - Stat
---

笔者没有学过数学分析等知识，所以全文的内容应该有很大一部分是不严格的。当成感性理解看吧。

# 统计学的基本框架

- 描述：通过一些参数来描述总体的特征。
- 推断：基于已有的数据（样本），推断总体的规律，甚至作出预测。

# 概率分布

对于一个连续型随机变量 $X$，若有一函数 $f$ 恒非负，且满足对于任意区间 $[l,r]$，$P(X \in [l,r]) = \int_l^r f(x) dx$，则称 $f$ 是 $X$ 的**概率密度函数（Probability density function, PDF）**。

所有事件的概率和为 $1$，因此 PDF 满足以下性质：

$$\int_{-\infty}^\infty f(x) dx = 1$$

**累积分布函数（Cumulative distribution function, CDF）** 是 PDF 的原函数：

$$F(x) = \int_{-\infty}^x f(x) dx$$

**概率分布**描述不同事件发生的可能性，每一种概率分布有自己的 PDF。若我们有一个分布 $D$，我们想说 $X$ 服从这个分布，可以用这个符号：

$$X \sim D$$

对于常见的分布，我们还有特别的记号，见后文的常见概率分布。

除了**连续概率分布**，还有**离散概率分布**。这就更简单了，把所有 $\int$ 改成 $\sum$ 就行咯。对于离散分布我们一般不说 PDF，而是叫做**概率质量函数（Probability mass function, PMF）**，但是意思是一样的。

## 期望 Expectation

$$\mathbb E[X] = \int_{-\infty}^\infty x f(x) dx$$

直觉建立：和物理学中的**重心**是相似的。

$$x_{\text{CM}} = \frac {\int x \rho(x) dx} {\int \rho(x) dx}$$

分母上的总质量在概率的语境下是总概率，永远是 $1$。

## 方差 Variance

（注：AP 课本使用 $\text{var}$，但是互联网上的其他资料均使用 $\text{Var}$。所以可能会混用。）

$$\text{Var}(X) = \mathbb E[(X - \mathbb E[X])^2]$$

> $\text{Var}(X)$ 还会记作 $\sigma^2$。$\sigma$ 是**标准差（standard deviation）**，定义为方差的非负平方根 $\sqrt{\text{Var}(X)}$。还有一种写法是 $\sigma = \text{SD}(X)$（AP 课本的写法）。
>
> 若 $X$ 满足期望为 $\mu$ 标准差为 $\sigma$ 的分布，则 $\frac {X - \mu} \sigma$ 满足期望为 $0$ 标准差为 $1$ 的分布。$\frac {X - \mu} \sigma$ 被称为 **Z 分数（z-score）**，也叫**标准分（standard score）**。

直觉建立：

$$\text{Var}(X) = \int (x - \mathbb E[X])^2 f(x) dx$$

和物理学中的**转动惯量**是相似的，这里特指轴过重心时的转动惯量。转动惯量和方差都描述了分布的分散程度，一个是物理的密度，另一个是概率的密度。

根据这样的直觉，我们可以发现**平行轴定理**的类比：

$$\mathbb E[(X - a)^2] = \text{Var}(X) + (\mathbb E[X] - a)^2$$

令 $a=0$ 我们可以得到一个方差的常用公式：

$$\text{Var}(X) = \mathbb E[X^2] - \mathbb E[X]^2$$

## 协方差 Covariance

仿照物理中**惯量积**（即 $\int xy dm = \int xy \rho(x,y) dx dy$）的定义，我们定义 $\text{Cov}(X,Y)$ 是 $X,Y$ 的**协方差**：

$$\text{Cov}(X,Y) = \int_{-\infty}^\infty \int_{-\infty}^\infty (x - \mathbb E[X]) (y - \mathbb E[Y]) f(x,y) dx dy$$

即

$$\text{Cov}(X,Y) = \mathbb E[(X - \mathbb E[X]) (Y - \mathbb E[Y])]$$

仿照前面的平行轴定理我们可以写出类似的公式：

$$\text{Cov}(X,Y) = \mathbb E[XY] - \mathbb E[X] \mathbb E[Y]$$

---

注意到，根据协方差定义，$\text{Cov}(X,Y) > 0$ 意味着当 $X > \mathbb E[X]$ 时经常 $Y > \mathbb E[Y]$，反之亦然，这体现了一种“正相关”性。同理 $\text{Cov}(X,Y) < 0$ 体现了一种“负相关性”。

但是用协方差来描述相关性不太好，因为带量纲。我们除掉两个标准差化为无量纲数，称为**总体 Pearson 相关系数**：

$$\rho_{X,Y} = \frac {\text{Cov}(X, Y)} {\text{SD}(X) \text{SD}(Y)}$$

取值是 $\rho_{X,Y} \in [-1, 1]$。当取到 $\pm 1$ 时，意味着每个点都在同一直线上（这一结论应当有简单证明，但是我把它放在最小二乘法一段中用决定系数证明）。$\rho_{X,Y}$ 也记作 $\text{Corr}(X,Y)$。

写开表达式：

$$\rho_{X,Y} = \frac {\sum (x_i - \bar x) (y_i - \bar y)} {\sqrt {\sum (x_i - \bar x)^2} \sqrt {\sum (y_i - \bar y)^2}}$$

我们发现其实这很像求向量夹角余弦值。事实上，我们定义 $\vec{x} = [x_1 - \bar x, \cdots]^T$ 和 $\vec{y} = [y_1 - \bar y, \cdots]^T$，则

$$\rho_{X,Y} = \frac {\vec x \cdot \vec y} {\lVert \vec x \rVert \lVert \vec y \rVert} = \cos \langle \vec x, \vec y \rangle$$

### 协方差为 $0$ 表示什么？

表示两个变量**没有线性的相关性**，但**不意味着独立**。一个直观理解是最小二乘法得到的最佳拟合直线斜率为 $0$，即**用线性的模型去拟合它们的关系时会认为它们没有关系**。

## 协方差矩阵

仿照物理中**转动惯量张量**，我们定义**协方差**矩阵（以二维为例）：

$$\Sigma = \begin{bmatrix}
    \text{Var}(X) & \text{Cov}(X,Y) \\
    \text{Cov}(Y,X) & \text{Var}(Y) \\
\end{bmatrix}$$

和转动惯量张量一样，对于一个单位向量 $\vec{u}$，我们可以得到这个方向上 $\vec{X}$ 的方差：

$$\text{Var}(\vec{u}^T \vec{X}) = \vec{u}^T \Sigma \vec{u}$$

## 期望和方差的常见性质

### 期望

- 对于两个随机变量 $X,Y$（**不要求独立**），有**期望的线性性**：

$$\mathbb E[aX + bY] = a \mathbb E[X] + b \mathbb E[Y]$$

- 对于两个**独立**的随机变量 $X,Y$，有：

$$\mathbb E[XY] = \mathbb E[X] \mathbb E[Y]$$

对于非独立的情况：$\mathbb E[XY] = \mathbb E[X] \mathbb E[Y] + \text{Cov}(X, Y)$

### 方差

- 平移一个常数 $C$：

$$\text{Var}(X + C) = \text{Var}(X)$$

- 数乘：

$$\text{Var}(aX) = a^2 \text{Var}(X)$$

- 对于**独立**的随机变量 $X,Y$，有：

$$\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y)$$

证明：

$$\begin{aligned}
    \text{Var}(X + Y) &= \mathbb E[(X+Y)^2] - \mathbb E[X+Y]^2 \\
    &= \mathbb E[X^2] + 2 \mathbb E[XY] + \mathbb E[Y^2] - \mathbb E[X]^2 - 2 \mathbb E[X] \mathbb E[Y] - \mathbb E[Y]^2 \\
    &= \text{Var}(X) + \text{Var}(Y) + 2 (\mathbb E[XY] - \mathbb E[X] \mathbb E[Y]) \\
    &= \text{Var}(X) + \text{Var}(Y) \\
\end{aligned}$$

对于非独立的情况：$\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) + 2 \text{Cov}(X, Y)$

### 协方差

- 可交换：

$$\text{Cov}(X,Y) = \text{Cov}(Y,X)$$

- 数乘：

$$\text{Cov}(aX, bY) = ab \text{Cov}(Y,X)$$

- $\text{Cov}$ 对加法的分配律：

$$\text{Cov}(X + Y, Z) = \text{Cov}(X, Z) + \text{Cov}(Y, Z)$$

## 条件期望

*这段是写完最小二乘法回来补的，可能有点突兀*。

$$\mathbb E[y \mid x]$$

是一个关于 $x$ 的函数，表示在 $x$ 确定时 $y$ 的期望值。

### 全期望公式

也叫期望迭代定理。这种叫法似乎更多，但是我喜欢全期望公式，可以和全概率公式对应上。英语叫 **Law of total expectation**。

$$\boxed{
    \mathbb E[Y] = \mathbb E[\mathbb E[Y \mid X]]
}$$

另一个说法，来自 [Wikipedia](https://en.wikipedia.org/wiki/Law_of_total_expectation)：对于样本空间的一个有限或可数无限的划分 $\{A_i\}$，有：

$$\mathbb E[Y] = \sum\limits_i \mathbb E[Y \mid A_i] P(A_i)$$

证明：

$$\begin{aligned}
    & \mathbb E[\mathbb E[Y \mid X]] \\
    =& \int \mathbb E[Y \mid X = x] f_X(x) dx \\
    =& \int \int y f_{Y \mid X}(y \mid X = x) dy f_X(x) dx \\
    =& \int y \left(\int f_{Y \mid X}(y \mid X = x) f_X(x) dx \right) dy \\
    =& \int y f_Y(y) dy \\
    =& \mathbb E[Y] \\
\end{aligned}$$

可以类比导数的**全导数公式**理解，类似全导数的 DAG（有向无环图）结构，全期望也有类似的形式。

## 常见的概率分布

见[另一篇文章](https://august-light.github.io/posts/2025/09/2025-09-03-%E5%B8%B8%E8%A7%81%E7%9A%84%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83/)。实践性较强，与本文的理论关系不大。

特别地，关于正态分布的介绍在[这个链接](https://august-light.github.io/posts/2025/09/2025-09-03-%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83/)。

## References

- [概率论学习笔记（一）](https://zhuanlan.zhihu.com/p/42859784)
- [概率论学习笔记（二）](https://zhuanlan.zhihu.com/p/42950143)
- [概率论学习笔记（四）](https://zhuanlan.zhihu.com/p/45720457)

# 总体和样本

刚才说到的 $\mu,\sigma$ 这些量，都是隐藏在现实世界背后的真正规律，是**总体**的量。在实际生活中，我们只能采集有限的样本，来试图逼近这些真实的总体的量。通过样本进行的估算，得到的是**样本**的量。我们使用记号 `\hat` 来说明一个总体量是被估算出来的，比如 $\hat \mu, \hat \sigma$。

通过保证期望正确来估计的方式称为**无偏估计**。例如我们可以限制 $\mathbb E[\hat{\mu}] = \mu$，$\mathbb E[\hat{\sigma}^2] = \sigma^2$。

符号滥用：这一部分会混用 $x$ 和 $X$。

## 均值

总体均值记作 $\mu$，**样本均值**记作 $\bar{x}$（$\hat \mu$）。

接下来我们推导无偏估计 $\hat \mu$ 的表达式。令 $X_i$ 为 $X$ 的第 $i$ 次实现值。

$$\begin{aligned}
    \mu =& \mathbb E[X] \\
    =& \frac 1 n \sum\limits_{i=1}^n \mathbb E[X_i] \\
    =& \mathbb E \left[ \frac 1 n \sum\limits_{i=1}^n X_i \right] \\
\end{aligned}$$

$$\boxed{\hat \mu = \bar X = \frac 1 n \sum\limits_{i=1}^n X_i}$$

## 方差

总体方差记作 $\sigma^2$，**（无偏）样本方差**记作 $s^2$（$\hat \sigma^2$）。

$\sigma$ 是用 $\mu$ 计算的，$\hat \sigma$ 得要转成用 $\bar{x}$ 计算的形式。

$$\begin{aligned}
    & \sigma^2 \\
    =& \mathbb E \left[ \sum (x - \mu)^2 \right] \\
    =& \mathbb E \left[ \frac 1 n \sum (X_i - \mu)^2 \right] \\
    =& \frac 1 n \mathbb E \left[ \sum (X_i - \bar{X} + \bar{X} - \mu)^2 \right] \\
    =& \frac 1 n \mathbb E \left[ \sum (X_i - \bar{X})^2 + 2 (\bar{X} - \mu) \sum (X_i - \bar{X}) + \sum (\bar{X} - \mu)^2 \right] \\
    =& \frac 1 n \mathbb E \left[ \sum (X_i - \bar{X})^2 + n (\bar{X} - \mu)^2 \right] \\
    =& \frac 1 n \mathbb E \left[ \sum (X_i - \bar{X})^2 \right] + \frac 1 n \times n \times \mathbb E[(\bar{X} - \mathbb E[\bar{X}])^2] & ^* \mu = \mathbb E[\bar{X}] \\
    =& \frac 1 n \mathbb E \left[ \sum (X_i - \bar{X})^2 \right] + \frac 1 n \times n \times \text{Var}(\bar{X}) \\
    =& \frac 1 n \mathbb E \left[ \sum (X_i - \bar{X})^2 \right] + \frac {\sigma^2} n \\
\end{aligned}$$

因此 $\frac 1 n \mathbb E \left[ \sum (X_i - \bar{X})^2 \right] = \frac {n-1} n \sigma^2$，即

$$\sigma^2 = \mathbb E \left[ \frac 1 {n-1} \sum (X_i - \bar{X})^2 \right]$$

$$\boxed{\hat{\sigma}^2 = s^2 = \frac 1 {n-1} \sum\limits_{i=1}^n (X_i - \bar{X})^2}$$

这个除以 $n-1$ 初次见到可能有点反直觉。如果不是除以 $n-1$ 而是除以 $n$，这样得到的方差估计是**有偏样本方差**。这样算出来的有偏估计 $s_{\text{biased}}^2$ 满足 $\mathbb E[s_{\text{biased}}^2] = \frac{n-1}{n} \sigma^2$。在“似然”一节中，我们会讨论有偏样本方差的价值。

## 协方差

样本方差要除以 $n-1$，可以类比得到（无偏）样本协方差也要除以 $n-1$：

$$\boxed{
    \widehat{\text{Cov}}(X, Y) = \frac 1 {n-1} \sum\limits_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})
}$$

同理，ChatGPT 说除以 $n$ 的版本得到的有偏协方差是正态分布数据下 MLE 的。

### 样本 Pearson 相关系数

$$\hat{\rho}_{X,Y} = r = \frac {\widehat{\text{Cov}}(X,Y)} {\widehat{\text{SD}}(X) \widehat{\text{SD}}(Y)}$$

或者一个更常见的形式：

$$r = \frac {\sum\limits_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})} {\sqrt{\sum\limits_{i=1}^n (x_i - \bar{x})^2 \sum\limits_{i=1}^n (y_i - \bar{y})^2}}$$

AP 书上写的版本是：

$$r = \frac 1 {n-1} \sum\limits_{i=1}^n \left( \frac {x_i - \bar{x}} {\widehat{\text{SD}}(X)} \right) \left( \frac {y_i - \bar{y}} {\widehat{\text{SD}}(Y)} \right)$$

本质相同，但是是从 Z 分数的视角来看的。

# 估计

## 有偏样本方差是没有用的吗？

有偏方差连期望都不对，虽然公式很好看，但是它难道不是错的吗？

介绍一个概念叫做**似然（likelihood）**。我们已经通过实验收集了一个样本，现在我们想知道当参数取某个值时，出现这个样本的概率为多大，这个概率称为似然。

比如，掷了 $10$ 次硬币有 $4$ 个正面，硬币正面概率为 $p$。似然：

$$L(p) = \binom {10} 4 p^4 (1 - p)^6$$

由此有一种思想叫做**最大似然估计（MLE）**，即已有一个观测数据出现时，找到最容易导致这个观测数据出现的参数。写成公式：

$$\hat{p} = \arg \max\limits_{p} L(p)$$

对于前面那个例子，可以算出 $\hat{p} = 0.4$，符合我们的直觉。

回到有偏样本方差。对于一组数据 $x_1, \cdots, x_n$ 来自 $\mathcal N(\mu, \sigma^2)$ i.i.d.，**在 MLE 意义下有偏方差是对总体方差的最好估计**。具体推导：

$$L(\mu, \sigma^2) = \prod\limits_{i=1}^n \frac 1 {\sqrt {2 \pi \sigma^2}} \exp\left( - \frac {(x_i - \mu)^2} {2 \sigma^2} \right)$$

为了方便计算，取对数：

$$\ln L (\mu, \sigma^2) = - \frac n 2 \ln (2 \pi \sigma^2) - \frac 1 {2 \sigma^2} \sum\limits_{i=1}^n (x_i - \mu)^2$$

（这一段机械计算略去）$\frac {\partial (\ln L)} {\partial \mu}$ 求一下可知 $\mu = \bar{x}$ 的时候是最优的，再求一下 $\frac {\partial (\ln L)} {\partial (\sigma^2)}$ 可知最优的是：

$$\sigma^2 = \frac 1 n \sum\limits_{i=1}^n (x_i - \bar{x})^2$$

也就是有偏的样本方差。

总之，**有偏的样本方差在数据满足正态分布 i.i.d. 时是满足 MLE 的**。这个结论在**最小二乘法**也有类似的形式。

TODO: 链接，这篇文章中还提到了 MLE 和 MoM 两种除了无偏以外的估计方法。