---
title: '最小二乘法'
date: 2025-09-30
permalink: /posts/2025/09/2025-09-30-最小二乘法/
excerpt: "介绍了最小二乘法的相关性质。"
tags:
  - Math
  - Stat
---

# 线性回归 - 最小二乘法

我有一堆数据 $(x_i, y_i)$，我想用一根直线 $y = a + b x$ 来拟合，即 $y_i = a + b x_i + u_i$（$u_i$ 是噪声），使得这条直线拟合得最好。即我们要找到参数 $\hat{a}, \hat{b}$，使得

$$y_i = \hat{a} + \hat{b} x_i + \hat{u}_i$$

中的噪声 $\hat{u}_i$ 要某种程度上尽量小。

噪声 $u_i$ 满足 $\mathbb E[u_i \mid x_i] = 0$（这并不意味着 $u_i$ 与 $x_i$ 独立！只是说 $x_i$ 不影响 $\mathbb E[u_i]$，且 $x_i$ 指定时 $\mathbb E[u_i] = 0$）。

## MLE 解法

增加一个假设：$u_i \sim \mathcal N(0, \sigma^2)$ i.i.d.。

仿照和样本方差有偏估计的方法，对似然函数求对数，我们可以发现最大化似然相当于最小化平方误差和（计算过程省略）：

$$\arg \min\limits_{a,b} \sum\limits_{i=1}^n (y_i - (a + b x_i))^2$$

求偏导：

$$\begin{cases}
    \sum\limits_{i=1}^n (y_i - \hat{a} - \hat{b} x_i) = 0 \\
    \sum\limits_{i=1}^n (y_i - \hat{a} - \hat{b} x_i) x_i = 0 \\
\end{cases}$$

> 补充一下，这两个式子能推出一些有趣的性质：
>
> - $\sum\limits_{i=1}^n \hat{u}_i = 0$。
> - $\sum\limits_{i=1}^n x_i \hat{u}_i = 0$。
>   - $\sum\limits_{i=1}^n y_i \hat{u}_i = 0$。
> - $\bar{\hat{y}} = \bar{y}$。
> - 点 $(\bar{x}, \bar{y})$ 一定在最优直线上，即 $\bar{y} = \hat{a} + \hat{b} \bar{x}$。

解方程组可得：

$$\boxed{\begin{cases}
    \hat{b} = \dfrac {\sum\limits_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})} {\sum\limits_{i=1}^n (x_i - \bar{x})^2} = \dfrac {\widehat {\text{Cov}}(x,y)} {\widehat {\text{Var}}(x)} \\
    \hat{a} = \bar{y} - \bar{x} \hat{b} \\
\end{cases}}$$

在 AP 考试中不涉及协方差，一般表示为：

$$\boxed{
    \hat{b} = r \frac {\widehat{\text{SD}}(y)} {\widehat{\text{SD}}(x)} = r \frac {s_y} {s_x}
}$$

其中 $r$ 是样本的 Pearson 相关系数。

## Method of Moments 解法

*tmd 这段居然是学校课上讲的东西。怎么这么难*。

MoM 的思想是：样本的某些矩应该等于总体的某些矩。通过强行让这些矩相等，可以解出参数。

它倒是和 MLE 没有关系，但是值得注意的是，对于**线性回归**它能够和 MLE 解出一样的解，且**不需要正态噪声假设**。

我们知道关于噪声的以下信息。根据 $\mathbb E[u \mid x] = 0$ 的假设，可以通过**全期望公式**得到 $\mathbb E[u] = 0$ 和 $\mathbb E[ux] = 0$：

$$\mathbb E[u] = \mathbb E[\mathbb E[u \mid x]] = 0$$

$$\mathbb E[ux] = \mathbb E[\mathbb E[ux \mid x]] = \mathbb E[x \mathbb E[u \mid x]] = 0$$

即对于任意 $i$，有：

$$\begin{cases}
    \mathbb E[u_i] = 0 \\
    \mathbb E[u_i x_i] = 0 \\
\end{cases}$$

这是总体的矩，让样本的矩与其相等，即为：

$$\begin{cases}
    \mathbb E[\hat{u}_i] = 0 \\
    \mathbb E[\hat{u}_i x_i] = 0 \\
\end{cases}$$

$$\begin{cases}
    \sum\limits_{i=1}^n (y_i - \hat{a} - \hat{b} x_i) = 0 \\
    \sum\limits_{i=1}^n (y_i - \hat{a} - \hat{b} x_i) x_i = 0 \\
\end{cases}$$

得到的其实就是刚才 MLE 中偏导得到的表达式，和最小二乘法相同。

### MoM 和 MLE 算出来一样，是巧合吗？

其实是巧合 hhh。

举一个算出来不一样的例子：有若干个 $x$ 独立取自均匀分布 $\mathcal U(0, \theta)$。现在要估算 $\theta$。
- MoM：$\mathbb E[x] = \frac \theta 2$，所以 $\hat \theta = 2 \bar{x}$。
- MLE：$L(\theta) = \prod\limits_{i=1}^n \frac {[0 \le x_i \le \theta]} \theta$，即 $\theta \ge \max\limits_{i=1}^n (x_i)$ 时是 $\theta^{-n}$，否则是 $0$。所以 $\hat \theta = \max(x_i)$。
- Unbiased：ChatGPT 说 $\hat \theta = \frac {n+1} n \max(x_i)$，我懒得算了。

**但是，线性回归问题中的 MoM、MLE 算出来的结果都是最小二乘法，同时最小二乘法还满足无偏**（$\hat a, \hat b$ 无偏的证明省略），由此体现了最小二乘的重要性。

## $b$ 的表达式为什么这么优雅？

先证明一个引理：$\text{Cov}(u,x) = 0$。

$$\begin{aligned}
    & \text{Cov}(u,x) \\
    =& \mathbb E[ux] - \mathbb E[u] \mathbb E[x] \\
    =& 0
\end{aligned}$$

接下来，因为结果中有协方差，我们求一下 $\text{Cov}(x,y)$：

$$\begin{aligned}
    & \text{Cov}(y,x) \\
    =& \text{Cov}(a,x) + \text{Cov}(bx,x) + \text{Cov}(u,x) \\
    =& b \text{Var}(x) & ^* \text{Cov}(u,x) = 0 \\
\end{aligned}$$

因此

$$b = \frac {\text{Cov}(x,y)} {\text{Var}(x)}$$

## 决定系数 $r^2$

我们现在希望找到一个量，能够衡量一个最小二乘的模型拟合的好不好（即**拟合优度**）。这个量称为**决定系数（coefficient of determination）**，用符号 $R^2$ 表示。

我们首先计算如下量：

- Sum of Squares Total: $SST = \sum\limits_{i=1}^n (y_i - \bar y)^2$
- Sum of Squares Regression: $SSR = \sum\limits_{i=1}^n (\hat y_i - y_i)^2$
- Sum of Squares Error: $SSE = \sum\limits_{i=1}^n (\hat y_i - \bar {\hat y})^2 = \sum\limits_{i=1}^n (\hat y_i - \bar y)^2$

我们认为以下值能够代表“模型能够解释样本的多少部分”：

$$R^2 = \frac {SSR} {SST}$$

那么剩下的 $SSE$ 有什么用呢？事实上，我们有以下结论：

$$SST = SSR + SSE$$

即：

$$\sum\limits_{i=1}^n (y_i - \bar y)^2 = \sum\limits_{i=1}^n (\hat y_i - \bar y)^2 + \sum\limits_{i=1}^n (\hat y_i - y_i)^2$$

因此我们也可以说

$$R^2 = 1 - \frac {SSE} {SST}$$

即用 $1$ 减掉没解释掉的比例。

### $SST = SSR + SSE$

尝试证明。注意到这个公式很像勾股定理，我们定义向量 $y = [y_1 \cdots y_n]^T$，$\bar y = [\bar y \cdots \bar y]^T$，$\hat y$ 同理。只要能够说明 $\hat y - \bar y$ **正交**于 $\hat y - y$，则套用勾股定理即得证。我们通过 dot product 为 $0$ 来说明正交：

$$\begin{aligned}
    & (\hat y - y) \cdot (\hat y - \bar y) \\
    =& (\hat y - y) \cdot \hat y - (\hat y - y) \cdot \bar y \\
\end{aligned}$$

$$\begin{aligned}
    & (\hat y - y_i) \cdot \hat y \\
    =& \sum (\hat y_i - y_i) \cdot \hat y_i \\
    =& \sum \hat u_i \hat y_i \\
    =& 0
\end{aligned}$$

$$\begin{aligned}
    & (\hat y - y_i) \cdot \bar y \\
    =& \bar y \sum (\hat y_i - y_i) \\
    =& \bar y \sum \hat u_i \\
    =& 0
\end{aligned}$$

向量 $\hat y - y_i$ 与向量 $\hat y$ 和 $\bar y$ 同时正交，因此与它们的差 $\hat y - \bar y$ 也正交。得证。

### $r^2$？

根据刚才的正交性推导，可知 $\hat y - \bar y$ 就是 $y - \bar y$ 的一个投影（即 $y - \bar y = (y - \hat y) + (\hat y - \bar y)$ 是一个正交分解），设夹角为 $\theta = \langle \hat y - \bar y, y - \bar y \rangle$。

$$R^2 = \frac {\lVert \hat y - \bar y \rVert^2} {\lVert y - \bar y \rVert^2} = \cos^2 \theta$$

说到 $\cos$，我们回想起 Pearson 相关系数也是一个 $\cos$：

$$\text{Corr}(x, y) = \cos \langle x - \bar x, y - \bar y \rangle$$

我们发现

$$\begin{aligned}
    & \cos^2 \theta \\
    =& \cos^2 \langle \hat y - \bar y, y - \bar y \rangle \\
    =& \cos^2 \langle \hat y - \bar {\hat y}, y - \bar y \rangle \\
    =& \text{Corr}(\hat y, y)^2 \\
\end{aligned}$$

对于一元线性回归的情况，我们发现 $\hat y$ 和 $x$ 是线性关系，因此 $\text{Corr}(\hat y, y) = \text{Corr}(x, y) = r$。因此，在一元线性回归中，决定系数 $R^2$ 也可以写作 $x,y$ 的相关系数 $r$ 的平方。

性质：当 $SSE = 0$，即完美解释时，$R^2 = 1$，对应 $r = \pm 1$，此时所有点都在同一直线上。

### 多元线性回归

在多元线性回归中，决定系数就与单个的 $x$ 没啥关系了，只能说 $R^2 = \text{Corr}(\hat y, y)^2$，所以一般会改回 $R^2$ 的符号。

$R^2$ 的一个性质：往模型中每多加一个参数 $x_i$，不管是有用信息还是噪声，$R^2$ 都单调不减。

# MLE 例题：Logistic Regression

在机器学习领域，除了最小二乘法用到 MLE 以外，Logistic 回归也是使用 MLE 的经典场景。

我有一堆数据 $(\vec{x}_i, y_i)$，其中 $y_i \in \{0,1\}$，代表 $\vec{x}_i$ 被归到哪类。我希望找到一个函数 $h_\theta(\vec{x})$，代表我预测的 $\vec{x}$ 被归类到 $1$ 的概率，而且使得 $h_\theta(\vec{x}_i)$ 尽量接近 $y_i$。

对于一组给定的参数 $\vec{\theta}$，我们预测 $\vec{x}$ 被归类为 $1$ 的概率为

$$h_\theta(\vec{x}) = \sigma(\vec{\theta}^T \vec{x}) = \frac 1 {1 + e^{-\vec{\theta}^T \vec{x}}}$$

（$\sigma$ 为 Sigmoid 函数 $\frac 1 {1 + e^{-x}}$，值域 $[0,1]$ 单调递增）

我们希望找到一个最好的 $\vec{\theta}$，最好用 MLE 定义。似然即为

$$L_i(\vec{\theta}) = \begin{cases}
    h_\theta(\vec{x}) & y_i = 1 \\
    1 - h_\theta(\vec{x}) & y_i = 0 \\
\end{cases}$$

$$L(\vec{\theta}) = \prod\limits_{i=1}^n L_i(\vec{\theta}) = \prod\limits_{i=1}^n h_\theta(\vec{x}_i)^{y_i} (1 - h_\theta(\vec{x}_i))^{1 - y_i}$$

为计算方便取对数：

$$\ln L(\vec{\theta}) = \sum\limits_{i=1}^n (y_i \ln h_\theta(\vec{x}_i) + (1 - y_i) \ln (1 - h_\theta(\vec{x}_i)))$$

这就是我们熟悉的 Logistic regression 的 Cost function 形式。