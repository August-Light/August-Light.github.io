---
title: 'Lesson 5 信息论与决策树'
date: 2026-01-29
permalink: /posts/2026/01/2026-01-29-Lesson-5-信息论与决策树/
excerpt: "概率论还在追我？？"
tags:
---

# 信息量

设事件「随机变量 $X$ 取到 $x$」的**信息量**记为 $I(x)$。一个事件的信息量应该只和它发生的概率 $p$ 有关，所以 $I(x)$ 应该也是一个关于 $p$ 的函数 $h(p)$。

根据我们对“信息量”的朴素直觉，多个**独立**事件同时发生的总信息量等于每个事件信息量的和：

$$h(p_1 p_2) = h(p_1) + h(p_2)$$

这是一个 Cauchy 方程，其解为

$$\boxed{
    h(p) = C \ln p
}$$

虽然信息量是一个无量纲数，但人们喜欢赋予一个单位来解释它：对于一个发生概率为 $\frac 1 2$ 的事件（比如抛硬币得到正面），可以用一个 $1$ 位的二进制数来标记它。我们认为这种事件所含的信息量为 $1$ **bit**。

$$\begin{aligned}
    C \ln \frac 1 2 &= 1 \text{ bit} \\
    C &= - \frac 1 {\ln 2} \text{ bit} \\
\end{aligned}$$

因此，在以 bit 作单位时：

$$\boxed{
    h(p) = - \log_2 p = \log_2 \frac 1 p
}$$

$$I(X) = - \log_2 P(X)$$

例子：
- 抛硬币的结果有 $1$ bit 信息。
- 抛四面骰子的结果有 $2$ bit 信息。
- 抛八面骰子的结果有 $3$ bit 信息。
- 抛六面骰子的结果有 $\log_2 6$ bit 信息。

## 条件信息量

概率有条件版本，信息量也有条件版本：

$$I(Y \mid X) = - \log_2 P(Y \mid X)$$

条件信息量也有 Bayes：

$$\begin{aligned}
    I(y \mid x) &= - \log_2 P(y \mid x) \\
    &= - \log_2 \frac {P(x \cap y)} {P(x)} \\
    &= (- \log_2 P(x \cap y)) - (- \log_2 P(x)) \\
    &= I(x,y) - I(x) \\
\end{aligned}$$

即：

$$I(Y \mid X) = I(X, Y) - I(X)$$

# 信息熵

一个随机变量 $X$ 能带来的信息量 $I(X)$ 的期望称为**信息熵** $H(X)$：

$$\boxed{\begin{aligned}
    H(X) &:= \mathbb E[I(X)] \\
    &= - \sum_x P(x) \log_2 P(x)
\end{aligned}}$$

特别地，一个概率为 $0$ 的事件对信息熵的贡献为：

$$\begin{aligned}
    & \lim_{p \to 0^+} - p \ln p \\
    =& \lim_{p \to 0^+} \frac {\ln \frac 1 p} {\frac 1 p} \\
    =& \lim_{t \to \infty} \frac {\ln t} t \\
    =& 0 \\
\end{aligned}$$

多个变量可以定义联合熵：

$$\begin{aligned}
    H(X,Y) &:= \mathbb E[I(X,Y)] \\
    &= - \sum_{x,y} P(x,y) \log_2 P(x,y) \\
\end{aligned}$$

## 条件信息熵

对 $I(Y \mid X)$ 用全期望公式：

$$\mathbb E[I(Y \mid X)] = \sum_x P(x) \mathbb E[I(Y \mid X = x)]$$

我们就得到了信息熵的 Bayes 公式：

$$\boxed{
    H(Y \mid X) = \sum_x P(x) H(Y \mid X = x)
}$$

我们还知道

$$\begin{aligned}
    I(Y \mid X) &= I(X, Y) - I(X) \\
    I(X,Y) &= I(Y \mid X) + I(X) \\
\end{aligned}$$

两边取期望：

$$\boxed{
    H(X,Y) = H(Y \mid X) + H(X) \\
}$$