---
title: 'Lesson 5 信息论与决策树'
date: 2026-01-29
permalink: /posts/2026/01/2026-01-29-Lesson-5-信息论与决策树/
excerpt: "概率论还在追我？？"
tags:
---

# 信息量

设事件「随机变量 $X$ 取到 $x$」的**信息量**记为 $I(x)$。一个事件的信息量应该只和它发生的概率 $p$ 有关，所以 $I(x)$ 应该也是一个关于 $p$ 的函数 $h(p)$。

根据我们对“信息量”的朴素直觉，多个**独立**事件同时发生的总信息量等于每个事件信息量的和：

$$h(p_1 p_2) = h(p_1) + h(p_2)$$

这是一个 Cauchy 方程，其解为

$$\boxed{
    h(p) = C \ln p
}$$

虽然信息量是一个无量纲数，但人们喜欢赋予一个单位来解释它：对于一个发生概率为 $\frac 1 2$ 的事件（比如抛硬币得到正面），可以用一个 $1$ 位的二进制数来标记它。我们认为这种事件所含的信息量为 $1$ **bit**。

$$\begin{aligned}
    C \ln \frac 1 2 &= 1 \text{ bit} \\
    C &= - \frac 1 {\ln 2} \text{ bit} \\
\end{aligned}$$

因此，在以 bit 作单位时：

$$\boxed{
    h(p) = - \log_2 p = \log_2 \frac 1 p
}$$

$$I(X) = - \log_2 P(X)$$

例子：
- 抛硬币的结果有 $1$ bit 信息。
- 抛四面骰子的结果有 $2$ bit 信息。
- 抛八面骰子的结果有 $3$ bit 信息。
- 抛六面骰子的结果有 $\log_2 6$ bit 信息。

当然，以 $2$ 为底在数学上是不自然的。因此也有以 $e$ 为底的信息量 $I(X) = - \ln P(X)$，此时单位由 bit 改为 nat。

## 条件信息量

概率有条件版本，信息量也有条件版本：

$$I(Y \mid X) = - \log_2 P(Y \mid X)$$

条件信息量也有 Bayes：

$$\begin{aligned}
    I(y \mid x) &= - \log_2 P(y \mid x) \\
    &= - \log_2 \frac {P(x \cap y)} {P(x)} \\
    &= (- \log_2 P(x \cap y)) - (- \log_2 P(x)) \\
    &= I(x,y) - I(x) \\
\end{aligned}$$

即：

$$I(Y \mid X) = I(X, Y) - I(X)$$

# 信息熵

一个随机变量 $X$ 能带来的信息量 $I(X)$ 的期望称为**信息熵** $H(X)$：

$$\boxed{\begin{aligned}
    H(X) &:= \mathbb E[I(X)] \\
    &= - \sum_x P(x) \log_2 P(x)
\end{aligned}}$$

特别地，一个概率为 $0$ 的事件对信息熵的贡献为：

$$\begin{aligned}
    & \lim_{p \to 0^+} - p \ln p \\
    =& \lim_{p \to 0^+} \frac {\ln \frac 1 p} {\frac 1 p} \\
    =& \lim_{t \to \infty} \frac {\ln t} t \\
    =& 0 \\
\end{aligned}$$

信息熵可以衡量一个随机变量的不确定性。

- 信息熵的最小值很容易想到：一个永远只能抛出正面的硬币（即确定事件），它的信息熵是 $0$。
- 最大值呢？

> 对于一个可灌铅的 $n$ 面骰子，应该怎么分配概率使得它的结果 $X$ 的信息熵 $H(X)$ 最大？

记 $h(p) = - p \log_2 p$。

我们希望找一个不等式限制住 $H(X) = \sum_{i=1}^n h(p_i)$，与此同时保持 $p_i \ge 0$ 和 $\sum_{i=1}^n p_i = 1$。

容易想到 Jensen 不等式：

$$\frac 1 n \sum_{i=1}^n h(p_i) \le h\left( \frac 1 n \sum_{i=1}^n p_i \right)$$

（计算一下会发现在 $p > 0$ 时 $h''(p) < 0$，即 $h$ 在 $(0, \infty)$ 上严格凹，确实满足 Jensen 不等式的使用条件）

$$\begin{aligned}
    H(X) = \sum_{i=1}^n h(p_i) & \le n h(\frac 1 n) \\
    &= \log_2 n
\end{aligned}$$

在 $p_1 = \cdots = p_n = \frac 1 n$ 时取等。

因此，信息熵的最大值在均匀分布取到，为 $H(X) = \log_2 n$。

感性理解：骰子没有偏好才最难预测，当然不确定性最大。

## 联合信息熵

多个变量可以定义联合信息熵：

$$\begin{aligned}
    H(X,Y) &:= \mathbb E[I(X,Y)] \\
    &= - \sum_{x,y} P(x,y) \log_2 P(x,y) \\
\end{aligned}$$

## 条件信息熵

对 $I(Y \mid X)$ 用全期望公式：

$$\mathbb E[I(Y \mid X)] = \sum_x P(x) \mathbb E[I(Y \mid X = x)]$$

我们就得到了信息熵的 Bayes 公式：

$$\boxed{
    H(Y \mid X) = \sum_x P(x) H(Y \mid X = x)
}$$

我们还知道

$$\begin{aligned}
    I(Y \mid X) &= I(X, Y) - I(X) \\
    I(X,Y) &= I(Y \mid X) + I(X) \\
\end{aligned}$$

两边取期望：

$$\boxed{
    H(X,Y) = H(Y \mid X) + H(X) \\
}$$

## 信息增益

$X$ 对 $Y$ 造成的信息增益，就是看知道了 $X$ 之后 $Y$ 的不确定性下降了多少：

$$\boxed{
    IG(Y, X) := H(Y) - H(Y \mid X)
}$$

> 有一个论坛。我们要考察用户的**性别**和**活跃度**哪个对流失率影响更大。
>
> 我们有 $15$ 个样本，$5$ 个正样本（确实流失了），$10$ 个负样本。
>
> - 按性别分类：
>   - 男：$3$ 正 $5$ 负
>   - 女：$2$ 正 $5$ 负
> - 按活跃度分类：
>   - 高：$0$ 正 $6$ 负
>   - 中：$1$ 正 $4$ 负
>   - 低：$4$ 正 $0$ 负

令随机变量 $Y \in \{1, 0\}$ 表示正 / 负样本。

计算 $Y$ 的信息熵：

$$\begin{aligned}
    H(Y) &= - \sum_y P(y) \log_2 P(y) \\
    &= - (\frac 5 {15} \log_2 \frac 5 {15} + \frac {10} {15} \log_2 \frac {10} {15}) \\
    & \approx 0.918 \text{ bit}
\end{aligned}$$

计算在已知性别时的 $Y$ 的条件信息熵：

$$\begin{aligned}
    H(Y \mid \text{male}) &= - \sum_y P(y \mid \text{male}) \log_2 \sum_y P(y \mid \text{male}) \\
    &= - (\frac 3 8 \log_2 \frac 3 8 + \frac 5 8 \log_2 \frac 5 8) \\
    & \approx 0.954 \text{ bit}
\end{aligned}$$

$$H(Y \mid \text{female}) = \cdots \approx 0.863 \text{ bit}$$

$$\begin{aligned}
    H(Y \mid \text{gender}) &= \sum_x P(x) H(Y \mid X = x) \\
    &= \frac 8 {15} H(Y \mid \text{male}) + \frac 7 {15} H(Y \mid \text{female}) \\
    & \approx 0.912 \text{ bit}
\end{aligned}$$

计算性别这一特征带来的信息增益：

$$\begin{aligned}
    IG(Y, \text{gender}) &= H(Y) - H(Y \mid \text{gender}) \\
    & \approx 0.918 \text{ bit} - 0.912 \text{ bit} \\
    & \approx 0.006 \text{ bit}
\end{aligned}$$

通过同样的计算：

$$H(Y \mid \text{activity}) \approx 0.241 \text{ bit}$$

$$\begin{aligned}
    IG(Y, \text{activity}) &= H(Y) - H(Y \mid \text{activity}) \\
    & \approx 0.918 \text{ bit} - 0.241 \text{ bit} \\
    & \approx 0.677 \text{ bit}
\end{aligned}$$

结论：活跃度带来的信息增益远大于性别。因此活跃度带来的影响更大。

# ID3 决策树

这是一个**决策树 (Decision tree)** 的例子：

<img src="https://images.edrawmax.com/images/knowledge/decision-tree-buy-car.png" width=400>

> 用 $X_1, \cdots, X_n$ 这 $n$ 个特征，制造一棵决策树来预测 $Y$。怎样能够比较好地限制这棵树的大小？
>
> e.g. 用性别和活跃度预测用户是否流失。

我们每次贪心地挑选信息增益最大的特征，然后递归建树。这被称为 **ID3 算法**（ID3 是 Iterative Dichotomiser 3 的缩写）。

# C4.5 决策树

TODO:

# CART 决策树

TODO: